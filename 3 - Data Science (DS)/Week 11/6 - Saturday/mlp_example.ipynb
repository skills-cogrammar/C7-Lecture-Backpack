{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "\n",
    "**With forward propagation only**\n",
    "\n",
    "This notebook implements a neural network from scratch to gain an understanding of how neural networks work, and that is essential for designing effective models. We create a multilayer perceptron i.e. a neural network with input, hidden, and output layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use \n",
    "1. One input layer with variable number of input nodes \n",
    "2. Variable number of hidden layers with variable number of nodes in each\n",
    "3. One output layer with variable output nodes\n",
    "\n",
    "The default for the MLP class below is set to 3 input nodes, 2 hidden layers with 3 nodes in each hidden layer, and 2 output nodes. \n",
    "\n",
    "\n",
    "**Note: Forward propagation only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a MLP class.\n",
    "class MLP(object):\n",
    "\n",
    "    \"\"\"A Multilayer Perceptron class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs=3, hidden_layers=[3, 3], num_outputs=2):\n",
    "        \"\"\" MLP constructor. Takes the number of inputs, a variable number of hidden layers, and number of outputs\n",
    "\n",
    "        Args:\n",
    "            num_inputs (int): Number of inputs\n",
    "            hidden_layers (list): A list of ints for the hidden layers\n",
    "            num_outputs (int): Number of outputs\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        layers = [num_inputs] + hidden_layers + [num_outputs]\n",
    "        print(\"Number of input, hidden, and outout nodes: \", [num_inputs], hidden_layers, [num_outputs])\n",
    "\n",
    "        # create random connection weights for the layers\n",
    "        weights = []\n",
    "        for i in range(len(layers)-1):\n",
    "            w = np.random.rand(layers[i], layers[i+1])\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "        \"\"\"Computes forward propagation of the network based on input signals.\n",
    "\n",
    "        Args:\n",
    "            inputs (ndarray): Input signals\n",
    "        Returns:\n",
    "            activations (ndarray): Output values\n",
    "        \"\"\"\n",
    "\n",
    "        # the input layer activation is just the input itself\n",
    "        activations = inputs\n",
    "\n",
    "        # iterate through the network layers\n",
    "        for w in self.weights:\n",
    "\n",
    "            # calculate matrix multiplication between previous activation and weight matrix\n",
    "            net_inputs = np.dot(activations, w)\n",
    "\n",
    "            # apply sigmoid activation function\n",
    "            activations = self._sigmoid(net_inputs)\n",
    "\n",
    "        # return output layer activation\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\n",
    "        Args:\n",
    "            x (float): Value to be processed\n",
    "        Returns:\n",
    "            y (float): Output\n",
    "        \"\"\"\n",
    "        \n",
    "        y = 1.0 / (1 + np.exp(-x))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input, hidden, and outout nodes:  [4] [3, 2, 3] [2]\n",
      "Inputs: [0.34005629 0.67085002 0.18275496 0.49313703]\n",
      "Network activation: [0.74384842 0.70828567]\n"
     ]
    }
   ],
   "source": [
    "# create a Multilayer Perceptron, change the default number of hidden layers, and/or the default number of nodes in each layer\n",
    "mlp = MLP(num_inputs=4, hidden_layers=[3, 2, 3], num_outputs=2)\n",
    "\n",
    "# set random values for network's input\n",
    "inputs = np.random.rand(mlp.num_inputs)\n",
    "\n",
    "# perform forward propagation\n",
    "output = mlp.forward_propagate(inputs)\n",
    "\n",
    "print(\"Inputs: {}\".format(inputs))\n",
    "print(\"Network activation: {}\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of an MLP with only forward propagation. Next, we will look at the same code with backpropagation, minimising the error and updating the weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
