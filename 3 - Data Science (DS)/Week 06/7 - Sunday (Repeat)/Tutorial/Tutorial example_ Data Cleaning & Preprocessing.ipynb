{"cells":[{"cell_type":"markdown","metadata":{"id":"omawwBRKaaS6"},"source":["## Data Cleaning & Data Preprocessing\n"]},{"cell_type":"code","source":["! python -m spacy download en_core_web_s"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHqu53lnacZe","executionInfo":{"status":"ok","timestamp":1713531245312,"user_tz":-120,"elapsed":3819,"user":{"displayName":"Pierre Roodman","userId":"08657076676700421712"}},"outputId":"587303a0-eef7-4838-8cad-26909bd0385c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\u001b[38;5;1m✘ No compatible package found for 'en_core_web_s' (spaCy v3.7.4)\u001b[0m\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"BExgnSCNj06P"},"source":["\n","\n","Data scientists spend a large amount of their time cleaning datasets and getting them down to a form with which they can work. In fact, a lot of data scientists argue that the initial steps of obtaining and cleaning data constitute 80% of the job. It is important to be able to deal with messy data, whether that means missing values, inconsistent formatting, malformed records, or nonsensical outliers.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bQsqVKRj06T","outputId":"b391c8b6-5959-4b5e-b147-24bb57fee034"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Balance</th>\n","      <th>Income</th>\n","      <th>Limit</th>\n","      <th>Rating</th>\n","      <th>Cards</th>\n","      <th>Age</th>\n","      <th>Education</th>\n","      <th>Gender</th>\n","      <th>Student</th>\n","      <th>Married</th>\n","      <th>Ethnicity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12.240798</td>\n","      <td>14.891</td>\n","      <td>3606</td>\n","      <td>283</td>\n","      <td>2</td>\n","      <td>34</td>\n","      <td>11</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Caucasian</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23.283334</td>\n","      <td>106.025</td>\n","      <td>6645</td>\n","      <td>483</td>\n","      <td>3</td>\n","      <td>82</td>\n","      <td>15</td>\n","      <td>Female</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>22.530409</td>\n","      <td>104.593</td>\n","      <td>7075</td>\n","      <td>514</td>\n","      <td>4</td>\n","      <td>71</td>\n","      <td>11</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>27.652811</td>\n","      <td>148.924</td>\n","      <td>9504</td>\n","      <td>681</td>\n","      <td>3</td>\n","      <td>36</td>\n","      <td>11</td>\n","      <td>Female</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16.893978</td>\n","      <td>55.882</td>\n","      <td>4897</td>\n","      <td>357</td>\n","      <td>2</td>\n","      <td>68</td>\n","      <td>16</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Caucasian</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     Balance   Income  Limit  Rating  Cards  Age  Education  Gender Student  \\\n","0  12.240798   14.891   3606     283      2   34         11    Male      No   \n","1  23.283334  106.025   6645     483      3   82         15  Female     Yes   \n","2  22.530409  104.593   7075     514      4   71         11    Male      No   \n","3  27.652811  148.924   9504     681      3   36         11  Female      No   \n","4  16.893978   55.882   4897     357      2   68         16    Male      No   \n","\n","  Married  Ethnicity  \n","0     Yes  Caucasian  \n","1     Yes      Asian  \n","2      No      Asian  \n","3      No      Asian  \n","4     Yes  Caucasian  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","\n","df = pd.read_csv('balance.txt', delim_whitespace=True)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"rIPYU4Mwj06V"},"source":["### Dropping Columns in a DataFrame\n","\n","Often, you’ll find that not all the categories of data in a dataset are useful to you. For example, you might have a dataset containing student information (name, grade, standard, parents’ names, and address) but want to focus on analysing student grades.\n","In this case, the address or parents’ names categories are not important to you. Retaining these unneeded categories will take up unnecessary space and potentially also bog down runtime.\n","\n","Pandas provides a handy way of removing unwanted columns or rows from a DataFrame with the `drop()` function. Let’s look at a simple example where we drop a number of columns from a DataFrame.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOF4ZgrEj06W"},"outputs":[],"source":["df.drop(['Limit','Age'], inplace=True, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"VfEUCwLkj06X"},"source":["Above, we defined a list that contains the names of all the columns we want to drop. Next, we call the `drop()` function on our object, passing in the inplace parameter as `True` and the axis parameter as `1`. This tells pandas that we want the changes to be made directly in our object and that it should look for the values to be dropped in the columns of the object.\n","\n","When we inspect the DataFrame again, we’ll see that the unwanted columns have been removed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuDu2zg5j06X","outputId":"4dd1184a-db24-4969-be6a-f2c613c8c165"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Balance</th>\n","      <th>Income</th>\n","      <th>Rating</th>\n","      <th>Cards</th>\n","      <th>Education</th>\n","      <th>Gender</th>\n","      <th>Student</th>\n","      <th>Married</th>\n","      <th>Ethnicity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12.240798</td>\n","      <td>14.891</td>\n","      <td>283</td>\n","      <td>2</td>\n","      <td>11</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Caucasian</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23.283334</td>\n","      <td>106.025</td>\n","      <td>483</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>Female</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>22.530409</td>\n","      <td>104.593</td>\n","      <td>514</td>\n","      <td>4</td>\n","      <td>11</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>27.652811</td>\n","      <td>148.924</td>\n","      <td>681</td>\n","      <td>3</td>\n","      <td>11</td>\n","      <td>Female</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16.893978</td>\n","      <td>55.882</td>\n","      <td>357</td>\n","      <td>2</td>\n","      <td>16</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Caucasian</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     Balance   Income  Rating  Cards  Education  Gender Student Married  \\\n","0  12.240798   14.891     283      2         11    Male      No     Yes   \n","1  23.283334  106.025     483      3         15  Female     Yes     Yes   \n","2  22.530409  104.593     514      4         11    Male      No      No   \n","3  27.652811  148.924     681      3         11  Female      No      No   \n","4  16.893978   55.882     357      2         16    Male      No     Yes   \n","\n","   Ethnicity  \n","0  Caucasian  \n","1      Asian  \n","2      Asian  \n","3      Asian  \n","4  Caucasian  "]},"execution_count":129,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"pI1hMFWtj06Y"},"source":["### Replace values\n","\n","Sometimes you would like to replace a value from your data set with another value. For example if you had data with categories such as ‘Ethnicity’ and we wanted to rename one category lets say, 'African American' to 'African'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGQBCK2Qj06Z","outputId":"ec5d1cbe-47f5-45ce-e646-921c3e1485bd"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Balance</th>\n","      <th>Income</th>\n","      <th>Rating</th>\n","      <th>Cards</th>\n","      <th>Education</th>\n","      <th>Gender</th>\n","      <th>Student</th>\n","      <th>Married</th>\n","      <th>Ethnicity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12.240798</td>\n","      <td>14.891</td>\n","      <td>283</td>\n","      <td>2</td>\n","      <td>11</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Caucasian</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23.283334</td>\n","      <td>106.025</td>\n","      <td>483</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>Female</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>22.530409</td>\n","      <td>104.593</td>\n","      <td>514</td>\n","      <td>4</td>\n","      <td>11</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>27.652811</td>\n","      <td>148.924</td>\n","      <td>681</td>\n","      <td>3</td>\n","      <td>11</td>\n","      <td>Female</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16.893978</td>\n","      <td>55.882</td>\n","      <td>357</td>\n","      <td>2</td>\n","      <td>16</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Caucasian</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>22.486178</td>\n","      <td>80.180</td>\n","      <td>569</td>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Caucasian</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>10.574516</td>\n","      <td>20.996</td>\n","      <td>259</td>\n","      <td>2</td>\n","      <td>12</td>\n","      <td>Female</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>African</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>14.576204</td>\n","      <td>71.408</td>\n","      <td>512</td>\n","      <td>2</td>\n","      <td>9</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>7.938090</td>\n","      <td>15.125</td>\n","      <td>266</td>\n","      <td>5</td>\n","      <td>13</td>\n","      <td>Female</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Caucasian</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>17.756965</td>\n","      <td>71.061</td>\n","      <td>491</td>\n","      <td>3</td>\n","      <td>19</td>\n","      <td>Female</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>African</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     Balance   Income  Rating  Cards  Education  Gender Student Married  \\\n","0  12.240798   14.891     283      2         11    Male      No     Yes   \n","1  23.283334  106.025     483      3         15  Female     Yes     Yes   \n","2  22.530409  104.593     514      4         11    Male      No      No   \n","3  27.652811  148.924     681      3         11  Female      No      No   \n","4  16.893978   55.882     357      2         16    Male      No     Yes   \n","5  22.486178   80.180     569      4         10    Male      No      No   \n","6  10.574516   20.996     259      2         12  Female      No      No   \n","7  14.576204   71.408     512      2          9    Male      No      No   \n","8   7.938090   15.125     266      5         13  Female      No      No   \n","9  17.756965   71.061     491      3         19  Female     Yes     Yes   \n","\n","   Ethnicity  \n","0  Caucasian  \n","1      Asian  \n","2      Asian  \n","3      Asian  \n","4  Caucasian  \n","5  Caucasian  \n","6    African  \n","7      Asian  \n","8  Caucasian  \n","9    African  "]},"execution_count":130,"metadata":{},"output_type":"execute_result"}],"source":["df.replace('African American','African').head(10)"]},{"cell_type":"markdown","metadata":{"id":"hWO7J_mfj06Z"},"source":["### Grouping Data\n","\n","Grouping data sets is frequently applied in data analysis. For example, grouping is used when we need the result in terms of various groups present in the data set. Pandas has in-built methods which can roll the data into various groups.\n","\n","In the below example we group the data by Ethnicity and then get the result for a specific Ethnic group."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecvyYh0Lj06a","outputId":"4acad52f-3ede-4615-8ce2-1644e3660852"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Balance</th>\n","      <th>Income</th>\n","      <th>Rating</th>\n","      <th>Cards</th>\n","      <th>Education</th>\n","      <th>Gender</th>\n","      <th>Student</th>\n","      <th>Married</th>\n","      <th>Ethnicity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>23.283334</td>\n","      <td>106.025</td>\n","      <td>483</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>Female</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>22.530409</td>\n","      <td>104.593</td>\n","      <td>514</td>\n","      <td>4</td>\n","      <td>11</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>27.652811</td>\n","      <td>148.924</td>\n","      <td>681</td>\n","      <td>3</td>\n","      <td>11</td>\n","      <td>Female</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>14.576204</td>\n","      <td>71.408</td>\n","      <td>512</td>\n","      <td>2</td>\n","      <td>9</td>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>Asian</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>19.218800</td>\n","      <td>80.616</td>\n","      <td>394</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>Female</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Asian</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Balance   Income  Rating  Cards  Education  Gender Student Married  \\\n","1   23.283334  106.025     483      3         15  Female     Yes     Yes   \n","2   22.530409  104.593     514      4         11    Male      No      No   \n","3   27.652811  148.924     681      3         11  Female      No      No   \n","7   14.576204   71.408     512      2          9    Male      No      No   \n","12  19.218800   80.616     394      1          7  Female      No     Yes   \n","\n","   Ethnicity  \n","1      Asian  \n","2      Asian  \n","3      Asian  \n","7      Asian  \n","12     Asian  "]},"execution_count":131,"metadata":{},"output_type":"execute_result"}],"source":["grouped = df.groupby('Ethnicity')\n","grouped.get_group('Asian').head()"]},{"cell_type":"markdown","metadata":{"id":"p-pLpLhAj06b"},"source":["### Dealing with inconsistent data entry\n","\n","To begin with, let us install a module that will help us clean our data set. Go to your command prompt or terminal and type `pip install fuzzywuzzy` or `pip3 install fuzzywuzzy`. You will also need to install `python-Levenshtein` and `chardet`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-B-FB8aj06b"},"outputs":[],"source":["#%pip install fuzzywuzzy\n","#%pip install chardet\n","# helpful libraries\n","import fuzzywuzzy\n","from fuzzywuzzy import process\n","import chardet\n","\n","# set seed for reproducibility\n","np.random.seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qrzsWGFj06b","outputId":"5baff39f-9266-494d-8ee8-42d2c3162a6b","scrolled":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>store_name</th>\n","      <th>store_email</th>\n","      <th>department</th>\n","      <th>income</th>\n","      <th>date_measured</th>\n","      <th>country</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Cullen/Frost Bankers, Inc.</td>\n","      <td>NaN</td>\n","      <td>Clothing</td>\n","      <td>$54438554.24</td>\n","      <td>14 July 2006</td>\n","      <td>UK</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Nordson Corporation</td>\n","      <td>NaN</td>\n","      <td>Tools</td>\n","      <td>$41744177.01</td>\n","      <td>3 December 2006</td>\n","      <td>united states of america</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Stag Industrial, Inc.</td>\n","      <td>NaN</td>\n","      <td>Beauty</td>\n","      <td>$36152340.34</td>\n","      <td>12 August 2003</td>\n","      <td>UNITED STATES</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>FIRST REPUBLIC BANK</td>\n","      <td>ecanadine3@fc2.com</td>\n","      <td>Automotive</td>\n","      <td>$8928350.04</td>\n","      <td>26 October 2006</td>\n","      <td>UK</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Mercantile Bank Corporation</td>\n","      <td>NaN</td>\n","      <td>Baby</td>\n","      <td>$33552742.32</td>\n","      <td>24 December 1973</td>\n","      <td>UK</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                   store_name         store_email  department  \\\n","0   1   Cullen/Frost Bankers, Inc.                 NaN    Clothing   \n","1   2          Nordson Corporation                 NaN       Tools   \n","2   3        Stag Industrial, Inc.                 NaN      Beauty   \n","3   4          FIRST REPUBLIC BANK  ecanadine3@fc2.com  Automotive   \n","4   5  Mercantile Bank Corporation                 NaN        Baby   \n","\n","         income     date_measured                   country  \n","0  $54438554.24      14 July 2006                       UK   \n","1  $41744177.01   3 December 2006  united states of america  \n","2  $36152340.34    12 August 2003             UNITED STATES  \n","3   $8928350.04   26 October 2006                       UK   \n","4  $33552742.32  24 December 1973                       UK   "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["income_df = pd.read_csv(\"store_income_data_example.csv\")\n","income_df.head()"]},{"cell_type":"markdown","metadata":{"id":"55sISapvj06b"},"source":["#### Text pre-processing\n","\n","For this exercise, we are  interested in cleaning up the \"country\" column to make sure there are no data entry inconsistencies in it. We could go through and check each row by hand and manually correct inconsistencies when we find them. But there's a more efficient way to do this!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itC3vtlfj06c","outputId":"000a9c61-c2c2-4c1b-ee78-bab9c868843f"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 34 unique countries\n"]},{"data":{"text/plain":["array(['UK ', 'united states of america', 'UNITED STATES', 'uk',\n","       ' United States of America', 'South Africa ', 'United States.',\n","       'United States', 'South Africa/', 'United States ',\n","       'United States of America', 'South Africa.', 'United Kingdom ',\n","       'United States of America ', 'United States of America/',\n","       'south africa', 'UK/', 'United Kingdom.', ' United Kingdom',\n","       ' South Africa', 'United Kingdom/', 'SOUTH AFRICA', ' UK',\n","       'united kingdom', 'UNITED KINGDOM', ' United States',\n","       'UNITED STATES OF AMERICA', 'South Africa', 'United States/',\n","       'united states', 'United States of America.', 'UK',\n","       'United Kingdom', 'UK.'], dtype=object)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["countries = income_df['country'].unique()\n","print(f\"There are {len(countries)} unique countries\")\n","countries"]},{"cell_type":"markdown","metadata":{"id":"dmYeHAAej06c"},"source":["Just looking at this, we can see some problems due to inconsistent data entry. Let us look at the first entry 'United States of America/' and 'United States of America.'. These are the same countries but the computer understands them as different. The data capturer must have mistyped this country a few times.\n","\n","The first thing we need to do is make everything lower case (We can change it back at the end if we'd like) and remove any white spaces at the beginning and end of cells. Inconsistencies in capitalizations and trailing white spaces are very common in text data and you can fix a good 80% of your text data entry inconsistencies by doing this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhgG3vHkj06c","outputId":"b376e27a-5cf4-40ac-b67a-d87226a2bf37","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 15 unique countries\n"]},{"data":{"text/plain":["array(['uk', 'united states of america', 'united states', 'south africa',\n","       'united states.', 'south africa/', 'south africa.',\n","       'united kingdom', 'united states of america/', 'uk/',\n","       'united kingdom.', 'united kingdom/', 'united states/',\n","       'united states of america.', 'uk.'], dtype=object)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# convert to lower case\n","income_df['country'] = income_df['country'].str.lower()\n","\n","# remove trailing white spaces\n","income_df['country'] = income_df['country'].str.strip()\n","\n","# Let us view the data\n","countries = income_df['country'].unique()\n","print(f\"There are {len(countries)} unique countries\")\n","countries"]},{"cell_type":"markdown","metadata":{"id":"Ql8lpo1Qj06d"},"source":["Alright, let's take another look at the country column and see if there's any more data cleaning we need to do.\n","\n","It does look like there are some remaining inconsistencies: 'united states of america/' and 'united states of america' should probably be the same.\n","\n","We are going to use a `fuzzywuzzy` package to help identify which string are closest to each other. This dataset is small enough that we could probably could correct errors by hand, but that approach doesn't scale well. (Would you want to correct a thousand errors by hand? What about ten thousand? Automating things as early as possible is generally a good idea. Plus, it’s fun! :)\n","\n","Fuzzy matching: The process of automatically finding text strings that are very similar to the target string. In general, a string is considered \"closer\" to another one the fewer characters you'd need to change if you were transforming one string into another. So \"apple\" and \"snapple\" are two changes away from each other (add \"s\" and \"n\") while \"in\" and \"on\" and one change away (replace \"i\" with \"o\"). You won't always be able to rely on fuzzy matching, but it will usually end up saving you at least a little time.\n","\n","Fuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we're going to get the ten strings from our list of countries that have the closest distance to \"uk\".\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGO_3vb7j06d","outputId":"1df90982-13bd-45ef-8d49-ceeca2bbc463"},"outputs":[{"data":{"text/plain":["[('uk', 100),\n"," ('uk/', 100),\n"," ('uk.', 100),\n"," ('south africa', 14),\n"," ('south africa/', 14),\n"," ('south africa.', 14),\n"," ('united states', 13),\n"," ('united states.', 13),\n"," ('united states/', 13),\n"," ('united kingdom', 12)]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# get the top 10 closest matches to \"united kingdom\"\n","matches = fuzzywuzzy.process.extract(\"uk\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n","\n","# take a look at them\n","matches"]},{"cell_type":"markdown","metadata":{"id":"ubqwDsHRj06e"},"source":["We can see that two of the items in the countries are very close to \"uk\": \"uk/\" and \"uk.\". Let's replace all rows in our country column that have a ratio of > 90 with \"uk\".\n","\n","To do this, we going to write a function. (It's a good idea to write a general purpose function you can reuse if you think you might have to do a specific task more than once or twice. This keeps you from having to copy and paste code too often, which saves time and can help prevent mistakes.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwIgxb87j06e"},"outputs":[],"source":["# function to replace rows in the provided column of the provided dataframe\n","# that match the provided string above the provided ratio with the provided string\n","def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n","    # get a list of unique strings\n","    strings = df[column].unique()\n","\n","    # get the top 10 closest matches to our input string\n","    matches = fuzzywuzzy.process.extract(string_to_match, strings,\n","                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n","\n","    # only get matches with a ratio > 90\n","    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n","\n","    # get the rows of all the close matches in our dataframe\n","    rows_with_matches = df[column].isin(close_matches)\n","\n","    # replace all rows with close matches with the input matches\n","    df.loc[rows_with_matches, column] = string_to_match\n","    # let us know the function's done\n","    print(\"All done!\")"]},{"cell_type":"markdown","metadata":{"id":"V1wgAjukj06e"},"source":["Now that we have a function, we can put it to the test!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zk_S3YrOj06e","outputId":"19c7cafa-3ea0-4637-a8cd-a7a0f5886e15"},"outputs":[{"name":"stdout","output_type":"stream","text":["All done!\n","All done!\n","All done!\n","All done!\n","All done!\n","   id                   store_name         store_email  department  \\\n","0   1   Cullen/Frost Bankers, Inc.                 NaN    Clothing   \n","1   2          Nordson Corporation                 NaN       Tools   \n","2   3        Stag Industrial, Inc.                 NaN      Beauty   \n","3   4          FIRST REPUBLIC BANK  ecanadine3@fc2.com  Automotive   \n","4   5  Mercantile Bank Corporation                 NaN        Baby   \n","\n","         income     date_measured                   country  \n","0  $54438554.24      14 July 2006                        uk  \n","1  $41744177.01   3 December 2006  united states of america  \n","2  $36152340.34    12 August 2003             united states  \n","3   $8928350.04   26 October 2006                        uk  \n","4  $33552742.32  24 December 1973                        uk  \n"]}],"source":["replace_matches_in_column(df=income_df, column='country', string_to_match=\"united kingdom\")\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"united states\")\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"united states of america\")\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"south africa\")\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"uk\")"]},{"cell_type":"markdown","metadata":{"id":"f3YJ3Vioj06f"},"source":["And now let's can check the unique values in our country column again and make sure we've tidied up \"uk\" correctly.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QCK1wdOj06f","outputId":"847fb775-c934-451e-f6cd-4c978abf6a4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 5 unique countries\n"]},{"data":{"text/plain":["array(['uk', 'united states of america', 'united states', 'south africa',\n","       'united kingdom'], dtype=object)"]},"execution_count":139,"metadata":{},"output_type":"execute_result"}],"source":["# get all the unique values in the 'country' column\n","countries = income_df['country'].unique()\n","\n","print(f\"There are {len(countries)} unique countries\")\n","countries\n"]},{"cell_type":"markdown","metadata":{"id":"QVcJ2INNj06f"},"source":["Now there is one thing left to do: note that the UK and the United Kingdom are the same country. We could use fuzzy logic to fix errors like this, but it can sometimes be risky - for example, 'United States' might match 'United Kingdom' better than 'UK'. It is important to exercise caution in such cases. To fix these errors, we can simply replace them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_lLgnX8j06f","outputId":"29092028-9824-4b69-e7c6-34083c9d2c05"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 3 unique countries\n"]},{"data":{"text/plain":["array(['united kingdom', 'united states', 'south africa'], dtype=object)"]},"execution_count":140,"metadata":{},"output_type":"execute_result"}],"source":["income_df.replace('uk', 'united kingdom', inplace=True)\n","income_df.replace('united states of america', 'united states', inplace=True)\n","\n","# get all the unique values in the 'country' column\n","countries = income_df['country'].unique()\n","\n","print(f\"There are {len(countries)} unique countries\")\n","countries"]},{"cell_type":"markdown","metadata":{"id":"46koGsZ8j06g"},"source":["### Working with Date and time\n","\n","Analysing datasets with dates and times is often very cumbersome. Months of different lengths, different distributions of weekdays and weekends, leap years, and the dreaded timezones are just a few things you may have to consider depending on your context. For this reason, Python has a data type specifically designed for dates and times called datetime."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P07x5RTmj06g","outputId":"f14ce283-baee-45b5-8739-95253eac2588"},"outputs":[{"data":{"text/plain":["0        14 July 2006\n","1     3 December 2006\n","2      12 August 2003\n","3     26 October 2006\n","4    24 December 1973\n","Name: date_measured, dtype: object"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# modules we'll use\n","from datetime import date\n","# print the first few rows of the date column\n","income_df.date_measured.head()\n"]},{"cell_type":"markdown","metadata":{"id":"wa7fycqKj06g"},"source":["Yep, those are dates! But just because we can understand that it doesnt mean that the computer understands them as so. Notice that at the bottom of the output of `head()`, you can see that it says that the data type of this column is \"object\".\n","\n","Pandas uses the \"object\" dtype for storing various types of data types, but most often when you see a column with the dtype \"object\" it will have strings in it.\n","\n","If you check the [pandas dtype documentation](https://pandas.pydata.org/docs/user_guide/basics.html#dtypes), you'll notice that there's also a specific datetime64 dtype. Because the dtype of our column is object rather than datetime64, we can tell that Python doesn't know that this column contains dates.\n","\n","We can also look at just the dtype of your column without printing the first few rows if we like:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1Zd3vh1j06g","outputId":"c23bbe99-5ceb-4c41-ab6d-76b763f56313"},"outputs":[{"data":{"text/plain":["dtype('O')"]},"execution_count":142,"metadata":{},"output_type":"execute_result"}],"source":["# check the data type of our date column\n","income_df['date_measured'].dtype"]},{"cell_type":"markdown","metadata":{"id":"mbLOfzibj06g"},"source":["You may have to check the [numpy documentation](https://numpy.org/doc/stable/reference/arrays.interface.html#arrays-interface) to match the letter code to the dtype of the object. \"O\" is the code for \"object\", so we can see that these two methods give us the same information.\n","\n","### Convert our date columns to datetime\n","\n","Now that we know that our date column isn't being recognised as a date, it's time to convert it so that it is recognised as a date. This is called \"parsing dates\" because we're taking in a string and identifying its component parts.\n","\n","We can tell pandas what the format of our dates are with a guide called as \"strftime directive\". You can find more information about these directives in the [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.Period.strftime.html). The basic idea is that you need to point out which parts of the date are where and what punctuation is between them. There are lots of possible parts of a date,\n","\n","\n","|Code\t|Meaning\t|Example |\n","|---------|-----------|---------------|\n","|%A\t |   Weekday as locale’s full name |\tWednesday |\n","|%a\t |   Weekday as locale’s abbreviated name |\tWed |\n","|%B\t |  Month as locale’s full name |\tJune |\n","|%d\t | Day of the month |\t06 |\n","|%m\t |   Month as a number |\t6 |\n","|%Y\t |  Four-digit year |\t2018 |\n","|%y\t |  Two-digit year |\t18 |\n","\n","\n","\n","Some examples:\n","\n","1/17/07 has the format \"%m/%d/%y\"\n","\n","17-1-2007 has the format \"%d-%m-%Y\"\n","\n","\n","Looking back up at the head of the date column in the landslides dataset, we can see that it's in the format \"month/day/two-digit year\", so we can use the same syntax as the first example to parse in our dates:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYgGES2_j06h","outputId":"fca633e1-b5b9-4df3-e7b9-87dd42869ffc"},"outputs":[{"data":{"text/plain":["0   2006-07-14\n","1   2006-12-03\n","2   2003-08-12\n","3   2006-10-26\n","4   1973-12-24\n","Name: date_parsed, dtype: datetime64[ns]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# create a new column, date_parsed, with the parsed dates\n","\n","\n","income_df['date_parsed'] = pd.to_datetime(income_df['date_measured'], format='%d %B %Y')\n","\n","\n","income_df['date_parsed'].head()\n"]},{"cell_type":"markdown","metadata":{"id":"bYjHLmTaj06h"},"source":["Now when you check the first few rows of the new column, you can see that the dtype is datetime64. You can also see that the dates have been slightly rearranged so that they fit the default order datetime objects (year-month-day)."]},{"cell_type":"markdown","metadata":{"id":"9Oly9Ww_j06h"},"source":["Now that our dates are parsed correctly, we can interact with them in useful ways.\n","\n","What if I run into an error with multiple date formats? While we're specifying the date format here, sometimes you'll run into an error when there are multiple date formats in a single column. If that happens, you can have pandas try to infer what the right date format should be. You can do that like so:\n","\n","`income_df['date_parsed'] = pd.to_datetime(income_df['Date'], infer_datetime_format=True)`\n","\n","Why don't you always use `infer_datetime_format = True`?\n","There are two big reasons not to always have pandas guess the time format.\n","The first is that pandas won't always been able to figure out the correct date format, especially if someone has gotten creative with data entry.\n","The second is that it's much slower than specifying the exact format of the dates."]},{"cell_type":"markdown","metadata":{"id":"ZhJ_vu3Nj06h"},"source":["**Bibliography**\n","1. Agarwal, M. (n.d.). Pythonic Data Cleaning With NumPy and Pandas. Retrieved April 23, 2019, from Real Python: https://realpython.com/python-data-cleaning-numpy-pandas/\n","2. Sethi, N. (2018). Data Cleaning: Parsing Dates. Retrieved from Data Driven Investor: https://medium.com/datadriveninvestor/data-cleaning-parsing-dates-34792fc4d6c8"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"vscode":{"interpreter":{"hash":"63d17dc58a06b6a6d4136fb13c245dafcf53668da37b1c3052c24d689135f5bb"}}},"nbformat":4,"nbformat_minor":0}